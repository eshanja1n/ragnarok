{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cff43cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from os import environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e605c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "import tiktoken\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "\n",
    "def count_tokens(text, model=\"cl100k_base\"):\n",
    "    \"\"\"\n",
    "    Count the number of tokens in the text using tiktoken.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to count tokens for\n",
    "        model (str): The tokenizer model to use (default: cl100k_base for GPT-4)\n",
    "        \n",
    "    Returns:\n",
    "        int: Number of tokens in the text\n",
    "    \"\"\"\n",
    "    encoder = tiktoken.get_encoding(model)\n",
    "    return len(encoder.encode(text))\n",
    "\n",
    "def bs4_extractor(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    \n",
    "    # Target the main article content for LangGraph documentation \n",
    "    main_content = soup.find(\"article\", class_=\"md-content__inner\")\n",
    "    \n",
    "    # If found, use that, otherwise fall back to the whole document\n",
    "    content = main_content.get_text() if main_content else soup.text\n",
    "    \n",
    "    # Clean up whitespace\n",
    "    content = re.sub(r\"\\n\\n+\", \"\\n\\n\", content).strip()\n",
    "    \n",
    "    return content\n",
    "\n",
    "def load_langgraph_docs():\n",
    "    \"\"\"\n",
    "    Load LangGraph documentation from the official website.\n",
    "    \n",
    "    This function:\n",
    "    1. Uses RecursiveUrlLoader to fetch pages from the LangGraph website\n",
    "    2. Counts the total documents and tokens loaded\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of Document objects containing the loaded content\n",
    "        list: A list of tokens per document\n",
    "    \"\"\"\n",
    "    print(\"Loading LangGraph documentation...\")\n",
    "\n",
    "    # Load the documentation \n",
    "    urls = [\"https://langchain-ai.github.io/langgraph/concepts/\",\n",
    "     \"https://langchain-ai.github.io/langgraph/how-tos/\",\n",
    "     \"https://langchain-ai.github.io/langgraph/tutorials/workflows/\",  \n",
    "     \"https://langchain-ai.github.io/langgraph/tutorials/introduction/\",\n",
    "     \"https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform/local-server/\",\n",
    "    ] \n",
    "\n",
    "    docs = []\n",
    "    for url in urls:\n",
    "\n",
    "        loader = RecursiveUrlLoader(\n",
    "            url,\n",
    "            max_depth=5,\n",
    "            extractor=bs4_extractor,\n",
    "        )\n",
    "\n",
    "        # Load documents using lazy loading (memory efficient)\n",
    "        docs_lazy = loader.lazy_load()\n",
    "\n",
    "        # Load documents and track URLs\n",
    "        for d in docs_lazy:\n",
    "            docs.append(d)\n",
    "\n",
    "    print(f\"Loaded {len(docs)} documents from LangGraph documentation.\")\n",
    "    print(\"\\nLoaded URLs:\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"{i+1}. {doc.metadata.get('source', 'Unknown URL')}\")\n",
    "    \n",
    "    # Count total tokens in documents\n",
    "    total_tokens = 0\n",
    "    tokens_per_doc = []\n",
    "    for doc in docs:\n",
    "        total_tokens += count_tokens(doc.page_content)\n",
    "        tokens_per_doc.append(count_tokens(doc.page_content))\n",
    "    print(f\"Total tokens in loaded documents: {total_tokens}\")\n",
    "    \n",
    "    return docs, tokens_per_doc\n",
    "\n",
    "def save_llms_full(documents):\n",
    "    \"\"\" Save the documents to a file \"\"\"\n",
    "\n",
    "    # Open the output file\n",
    "    output_filename = \"llms_full.txt\"\n",
    "\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        # Write each document\n",
    "        for i, doc in enumerate(documents):\n",
    "            # Get the source (URL) from metadata\n",
    "            source = doc.metadata.get('source', 'Unknown URL')\n",
    "            \n",
    "            # Write the document with proper formatting\n",
    "            f.write(f\"DOCUMENT {i+1}\\n\")\n",
    "            f.write(f\"SOURCE: {source}\\n\")\n",
    "            f.write(\"CONTENT:\\n\")\n",
    "            f.write(doc.page_content)\n",
    "            f.write(\"\\n\\n\" + \"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "    print(f\"Documents concatenated into {output_filename}\")\n",
    "\n",
    "def split_documents(documents):\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks for improved retrieval.\n",
    "    \n",
    "    This function:\n",
    "    1. Uses RecursiveCharacterTextSplitter with tiktoken to create semantically meaningful chunks\n",
    "    2. Ensures chunks are appropriately sized for embedding and retrieval\n",
    "    3. Counts the resulting chunks and their total tokens\n",
    "    \n",
    "    Args:\n",
    "        documents (list): List of Document objects to split\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of split Document objects\n",
    "    \"\"\"\n",
    "    print(\"Splitting documents...\")\n",
    "    \n",
    "    # Initialize text splitter using tiktoken for accurate token counting\n",
    "    # chunk_size=8,000 creates relatively large chunks for comprehensive context\n",
    "    # chunk_overlap=500 ensures continuity between chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=8000,  \n",
    "        chunk_overlap=500  \n",
    "    )\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    \n",
    "    print(f\"Created {len(split_docs)} chunks from documents.\")\n",
    "    \n",
    "    # Count total tokens in split documents\n",
    "    total_tokens = 0\n",
    "    for doc in split_docs:\n",
    "        total_tokens += count_tokens(doc.page_content)\n",
    "    \n",
    "    print(f\"Total tokens in split documents: {total_tokens}\")\n",
    "    \n",
    "    return split_docs\n",
    "\n",
    "def create_vectorstore(splits):\n",
    "    \"\"\"\n",
    "    Create a vector store from document chunks using SKLearnVectorStore.\n",
    "    \n",
    "    This function:\n",
    "    1. Initializes an embedding model to convert text into vector representations\n",
    "    2. Creates a vector store from the document chunks\n",
    "    \n",
    "    Args:\n",
    "        splits (list): List of split Document objects to embed\n",
    "        \n",
    "    Returns:\n",
    "        SKLearnVectorStore: A vector store containing the embedded documents\n",
    "    \"\"\"\n",
    "    print(\"Creating SKLearnVectorStore...\")\n",
    "    \n",
    "    # Initialize OpenAI embeddings\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    \n",
    "    # Create vector store from documents using SKLearn\n",
    "    persist_path = os.getcwd()+\"/sklearn_vectorstore.parquet\"\n",
    "    vectorstore = SKLearnVectorStore.from_documents(\n",
    "        documents=splits,\n",
    "        embedding=embeddings,\n",
    "        persist_path=persist_path   ,\n",
    "        serializer=\"parquet\",\n",
    "    )\n",
    "    print(\"SKLearnVectorStore created successfully.\")\n",
    "    \n",
    "    vectorstore.persist()\n",
    "    print(\"SKLearnVectorStore was persisted to\", persist_path)\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "905f27fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LangGraph documentation...\n",
      "Loaded 5 documents from LangGraph documentation.\n",
      "\n",
      "Loaded URLs:\n",
      "1. https://langchain-ai.github.io/langgraph/concepts/\n",
      "2. https://langchain-ai.github.io/langgraph/how-tos/\n",
      "3. https://langchain-ai.github.io/langgraph/tutorials/workflows/\n",
      "4. https://langchain-ai.github.io/langgraph/tutorials/introduction/\n",
      "5. https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform/local-server/\n",
      "Total tokens in loaded documents: 9375\n",
      "Documents concatenated into llms_full.txt\n",
      "Splitting documents...\n",
      "Created 6 chunks from documents.\n",
      "Total tokens in split documents: 9686\n",
      "Creating SKLearnVectorStore...\n",
      "SKLearnVectorStore created successfully.\n",
      "SKLearnVectorStore was persisted to /Users/eshanjain/Desktop/ragnarok/sklearn_vectorstore.parquet\n"
     ]
    }
   ],
   "source": [
    "# Load the documents\n",
    "documents, tokens_per_doc = load_langgraph_docs()\n",
    "\n",
    "# Save the documents to a file\n",
    "save_llms_full(documents)\n",
    "\n",
    "# Split the documents\n",
    "split_docs = split_documents(documents)\n",
    "\n",
    "# Create the vector store\n",
    "vectorstore = create_vectorstore(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7826c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 relevant documents\n",
      "https://langchain-ai.github.io/langgraph/tutorials/workflows/\n",
      "# Write the updated section to completed sections\n",
      "    return result.content\n",
      "\n",
      "@task\n",
      "def synthesizer(completed_sections: list[str]):\n",
      "    \"\"\"Synthesize full report from sections\"\"\"\n",
      "    final_report = \"\\n\\n---\\n\\n\".join(completed_sections)\n",
      "    return final_report\n",
      "\n",
      "@entrypoint()\n",
      "def orchestrator_worker(topic: str):\n",
      "    sections = orchestrator(topic).result()\n",
      "    section_futures = [llm_call(section) for section in sections]\n",
      "    final_report = synthesizer(\n",
      "        [section_fut.result() for section_fut \n",
      "\n",
      "--------------------------------\n",
      "\n",
      "https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform/local-server/\n",
      "Run a local serverÂ¶\n",
      "This guide shows you how to run a LangGraph application locally.\n",
      "PrerequisitesÂ¶\n",
      "Before you begin, ensure you have the following:\n",
      "\n",
      "An API key for LangSmith - free to sign up\n",
      "\n",
      "1. Install the LangGraph CLIÂ¶\n",
      "# Python >= 3.11 is required.\n",
      "\n",
      "pip install --upgrade \"langgraph-cli[inmem]\"\n",
      "\n",
      "2. Create a LangGraph app ðŸŒ±Â¶\n",
      "Create a new app from the new-langgraph-project-python template. This template demonstrates a single-node application you can extend with your own logic.\n",
      "langgraph new pa\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "https://langchain-ai.github.io/langgraph/tutorials/workflows/\n",
      "Workflows and AgentsÂ¶\n",
      "This guide reviews common patterns for agentic systems. In describing these systems, it can be useful to make a distinction between \"workflows\" and \"agents\". One way to think about this difference is nicely explained in Anthropic's Building Effective Agents blog post:\n",
      "\n",
      "Workflows are systems where LLMs and tools are orchestrated through predefined code paths.\n",
      "Agents, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining \n",
      "\n",
      "--------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create retriever to get relevant documents (k=3 means return top 3 matches)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "    \n",
    "# Get relevant documents for the query\n",
    "query = \"What is LangGraph?\"    \n",
    "relevant_docs = retriever.invoke(query)\n",
    "print(f\"Retrieved {len(relevant_docs)} relevant documents\")\n",
    "\n",
    "for d in relevant_docs:\n",
    "    print(d.metadata['source'])\n",
    "    print(d.page_content[0:500])\n",
    "    print(\"\\n--------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c81cf9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def langgraph_query_tool(query: str):\n",
    "    \"\"\"\n",
    "    Query the LangGraph documentation using a retriever.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The query to search the documentation with\n",
    "\n",
    "    Returns:\n",
    "        str: A str of the retrieved documents\n",
    "    \"\"\"\n",
    "    retriever = SKLearnVectorStore(\n",
    "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\"), \n",
    "    persist_path=os.getcwd()+\"/sklearn_vectorstore.parquet\", \n",
    "    serializer=\"parquet\").as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "    relevant_docs = retriever.invoke(query)\n",
    "    print(f\"Retrieved {len(relevant_docs)} relevant documents\")\n",
    "    formatted_context = \"\\n\\n\".join([f\"==DOCUMENT {i+1}==\\n{doc.page_content}\" for i, doc in enumerate(relevant_docs)])\n",
    "    return formatted_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ed4e8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "[{'text': \"I'll help you understand what LangGraph is by searching the documentation.\", 'type': 'text'}, {'id': 'toolu_01FMcZUT7FjCyMUeEnTkRwUU', 'input': {'query': 'What is LangGraph'}, 'name': 'langgraph_query_tool', 'type': 'tool_use'}]\n",
      "Tool Calls:\n",
      "  langgraph_query_tool (toolu_01FMcZUT7FjCyMUeEnTkRwUU)\n",
      " Call ID: toolu_01FMcZUT7FjCyMUeEnTkRwUU\n",
      "  Args:\n",
      "    query: What is LangGraph\n"
     ]
    }
   ],
   "source": [
    "llm = ChatAnthropic(model=\"claude-3-7-sonnet-latest\", temperature=0)\n",
    "augmented_llm = llm.bind_tools([langgraph_query_tool])\n",
    "\n",
    "instructions = \"\"\"You are a helpful assistant that can answer questions about the LangGraph documentation. \n",
    "Use the langgraph_query_tool for any questions about the documentation.\n",
    "If you don't know the answer, say \"I don't know.\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": instructions},\n",
    "    {\"role\": \"user\", \"content\": \"What is LangGraph?\"}\n",
    "]\n",
    "\n",
    "message = augmented_llm.invoke(messages)\n",
    "message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf74854c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abffd2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "import tiktoken\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "class LangGraphRAG:\n",
    "    def __init__(self, vectorstore_path=None):\n",
    "        \"\"\"\n",
    "        Initialize the LangGraph RAG system.\n",
    "        \n",
    "        Args:\n",
    "            vectorstore_path (str): Path to existing vectorstore, if None will create new one\n",
    "        \"\"\"\n",
    "        self.vectorstore_path = vectorstore_path or os.path.join(os.getcwd(), \"sklearn_vectorstore.parquet\")\n",
    "        self.embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "        self.llm = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\", temperature=0)\n",
    "        self.vectorstore = None\n",
    "        self.retriever = None\n",
    "        \n",
    "        # Create the prompt template for RAG\n",
    "        self.prompt_template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a helpful assistant that answers questions about LangGraph documentation. \n",
    "Use the provided context to answer the user's question accurately and comprehensively.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Instructions:\n",
    "- Answer based primarily on the provided context\n",
    "- If the context doesn't contain enough information, say so clearly\n",
    "- Be specific and cite relevant details from the documentation\n",
    "- If you're unsure, acknowledge the uncertainty\"\"\"),\n",
    "            (\"human\", \"{question}\")\n",
    "        ])\n",
    "    \n",
    "    def count_tokens(self, text, model=\"cl100k_base\"):\n",
    "        \"\"\"Count the number of tokens in the text using tiktoken.\"\"\"\n",
    "        encoder = tiktoken.get_encoding(model)\n",
    "        return len(encoder.encode(text))\n",
    "\n",
    "    def bs4_extractor(self, html: str) -> str:\n",
    "        \"\"\"Extract text content from HTML using BeautifulSoup.\"\"\"\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        \n",
    "        # Target the main article content for LangGraph documentation \n",
    "        main_content = soup.find(\"article\", class_=\"md-content__inner\")\n",
    "        \n",
    "        # If found, use that, otherwise fall back to the whole document\n",
    "        content = main_content.get_text() if main_content else soup.text\n",
    "        \n",
    "        # Clean up whitespace\n",
    "        content = re.sub(r\"\\n\\n+\", \"\\n\\n\", content).strip()\n",
    "        \n",
    "        return content\n",
    "\n",
    "    def load_langgraph_docs(self):\n",
    "        \"\"\"Load LangGraph documentation from the official website.\"\"\"\n",
    "        print(\"Loading LangGraph documentation...\")\n",
    "\n",
    "        # Load the documentation \n",
    "        urls = [\n",
    "            \"https://langchain-ai.github.io/langgraph/concepts/\",\n",
    "            \"https://langchain-ai.github.io/langgraph/how-tos/\",\n",
    "            \"https://langchain-ai.github.io/langgraph/tutorials/workflows/\",  \n",
    "            \"https://langchain-ai.github.io/langgraph/tutorials/introduction/\",\n",
    "            \"https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform/local-server/\",\n",
    "        ] \n",
    "\n",
    "        docs = []\n",
    "        for url in urls:\n",
    "            loader = RecursiveUrlLoader(\n",
    "                url,\n",
    "                max_depth=5,\n",
    "                extractor=self.bs4_extractor,\n",
    "            )\n",
    "\n",
    "            # Load documents using lazy loading (memory efficient)\n",
    "            docs_lazy = loader.lazy_load()\n",
    "\n",
    "            # Load documents and track URLs\n",
    "            for d in docs_lazy:\n",
    "                docs.append(d)\n",
    "\n",
    "        print(f\"Loaded {len(docs)} documents from LangGraph documentation.\")\n",
    "        print(\"\\nLoaded URLs:\")\n",
    "        for i, doc in enumerate(docs):\n",
    "            print(f\"{i+1}. {doc.metadata.get('source', 'Unknown URL')}\")\n",
    "        \n",
    "        # Count total tokens in documents\n",
    "        total_tokens = 0\n",
    "        tokens_per_doc = []\n",
    "        for doc in docs:\n",
    "            doc_tokens = self.count_tokens(doc.page_content)\n",
    "            total_tokens += doc_tokens\n",
    "            tokens_per_doc.append(doc_tokens)\n",
    "        print(f\"Total tokens in loaded documents: {total_tokens}\")\n",
    "        \n",
    "        return docs, tokens_per_doc\n",
    "\n",
    "    def save_docs_to_file(self, documents, filename=\"llms_full.txt\"):\n",
    "        \"\"\"Save the documents to a file.\"\"\"\n",
    "        with open(filename, \"w\") as f:\n",
    "            for i, doc in enumerate(documents):\n",
    "                source = doc.metadata.get('source', 'Unknown URL')\n",
    "                f.write(f\"DOCUMENT {i+1}\\n\")\n",
    "                f.write(f\"SOURCE: {source}\\n\")\n",
    "                f.write(\"CONTENT:\\n\")\n",
    "                f.write(doc.page_content)\n",
    "                f.write(\"\\n\\n\" + \"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "        print(f\"Documents saved to {filename}\")\n",
    "\n",
    "    def split_documents(self, documents):\n",
    "        \"\"\"Split documents into smaller chunks for improved retrieval.\"\"\"\n",
    "        print(\"Splitting documents...\")\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=8000,  \n",
    "            chunk_overlap=500  \n",
    "        )\n",
    "        \n",
    "        split_docs = text_splitter.split_documents(documents)\n",
    "        \n",
    "        print(f\"Created {len(split_docs)} chunks from documents.\")\n",
    "        \n",
    "        total_tokens = sum(self.count_tokens(doc.page_content) for doc in split_docs)\n",
    "        print(f\"Total tokens in split documents: {total_tokens}\")\n",
    "        \n",
    "        return split_docs\n",
    "\n",
    "    def create_vectorstore(self, splits):\n",
    "        \"\"\"Create a vector store from document chunks.\"\"\"\n",
    "        print(\"Creating SKLearnVectorStore...\")\n",
    "        \n",
    "        vectorstore = SKLearnVectorStore.from_documents(\n",
    "            documents=splits,\n",
    "            embedding=self.embeddings,\n",
    "            persist_path=self.vectorstore_path,\n",
    "            serializer=\"parquet\",\n",
    "        )\n",
    "        print(\"SKLearnVectorStore created successfully.\")\n",
    "        \n",
    "        vectorstore.persist()\n",
    "        print(f\"SKLearnVectorStore persisted to {self.vectorstore_path}\")\n",
    "\n",
    "        return vectorstore\n",
    "\n",
    "    def load_vectorstore(self):\n",
    "        \"\"\"Load existing vectorstore from disk.\"\"\"\n",
    "        if os.path.exists(self.vectorstore_path):\n",
    "            print(f\"Loading existing vectorstore from {self.vectorstore_path}\")\n",
    "            self.vectorstore = SKLearnVectorStore(\n",
    "                embedding=self.embeddings,\n",
    "                persist_path=self.vectorstore_path,\n",
    "                serializer=\"parquet\"\n",
    "            )\n",
    "            self.retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Vectorstore not found at {self.vectorstore_path}\")\n",
    "            return False\n",
    "\n",
    "    def setup_rag_system(self, force_rebuild=False):\n",
    "        \"\"\"Set up the complete RAG system.\"\"\"\n",
    "        if not force_rebuild and self.load_vectorstore():\n",
    "            print(\"Using existing vectorstore.\")\n",
    "            return\n",
    "        \n",
    "        print(\"Building new vectorstore...\")\n",
    "        # Load documents\n",
    "        documents, _ = self.load_langgraph_docs()\n",
    "        \n",
    "        # Save documents to file (optional)\n",
    "        self.save_docs_to_file(documents)\n",
    "        \n",
    "        # Split documents\n",
    "        split_docs = self.split_documents(documents)\n",
    "        \n",
    "        # Create vectorstore\n",
    "        self.vectorstore = self.create_vectorstore(split_docs)\n",
    "        self.retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "    def retrieve_context(self, query: str) -> str:\n",
    "        \"\"\"Retrieve relevant context for a query.\"\"\"\n",
    "        if not self.retriever:\n",
    "            raise ValueError(\"RAG system not initialized. Call setup_rag_system() first.\")\n",
    "        \n",
    "        relevant_docs = self.retriever.invoke(query)\n",
    "        print(f\"Retrieved {len(relevant_docs)} relevant documents\")\n",
    "        \n",
    "        # Format context from retrieved documents\n",
    "        formatted_context = \"\\n\\n\".join([\n",
    "            f\"==DOCUMENT {i+1}==\\nSource: {doc.metadata.get('source', 'Unknown')}\\nContent: {doc.page_content}\" \n",
    "            for i, doc in enumerate(relevant_docs)\n",
    "        ])\n",
    "        \n",
    "        return formatted_context\n",
    "\n",
    "    def query(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        Query the RAG system and get an answer.\n",
    "        \n",
    "        Args:\n",
    "            question (str): The question to ask about LangGraph\n",
    "            \n",
    "        Returns:\n",
    "            str: The answer based on the retrieved context\n",
    "        \"\"\"\n",
    "        if not self.retriever:\n",
    "            raise ValueError(\"RAG system not initialized. Call setup_rag_system() first.\")\n",
    "        \n",
    "        # Retrieve relevant context\n",
    "        context = self.retrieve_context(question)\n",
    "        \n",
    "        # Generate response using the LLM\n",
    "        messages = self.prompt_template.format_messages(\n",
    "            context=context,\n",
    "            question=question\n",
    "        )\n",
    "        \n",
    "        response = self.llm.invoke(messages)\n",
    "        return response.content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "daa433f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing vectorstore from /Users/eshanjain/Desktop/ragnarok/sklearn_vectorstore.parquet\n",
      "Using existing vectorstore.\n",
      "\n",
      "==================================================\n",
      "LangGraph RAG System - Example Queries\n",
      "==================================================\n",
      "\n",
      "Q: What is LangGraph?\n",
      "------------------------------\n",
      "Retrieved 3 relevant documents\n",
      "A: Based on the provided context, LangGraph is a framework for building workflows and agents using LLMs (Large Language Models). Here are the key aspects of LangGraph:\n",
      "\n",
      "1. Core Functionality:\n",
      "- LangGraph allows you to build both workflows (systems where LLMs and tools are orchestrated through predefined code paths) and agents (systems where LLMs dynamically direct their own processes and tool usage)\n",
      "- It provides support for common patterns in agentic systems\n",
      "\n",
      "2. Key Benefits:\n",
      "- Persistence: Supports human-in-the-loop interactions and both short-term and long-term memory\n",
      "- Streaming: Provides multiple ways to stream workflow/agent outputs or intermediate state\n",
      "- Deployment: Offers easy on-ramp for deployment, observability, and evaluation\n",
      "\n",
      "3. Common Workflow Patterns Supported:\n",
      "- Prompt Chaining: Sequential LLM calls where each processes the output of the previous one\n",
      "- Parallelization: Multiple LLMs working simultaneously on tasks\n",
      "- Routing: Classifying inputs and directing them to specialized followup tasks\n",
      "- Orchestrator-Worker: A central LLM breaking down tasks and delegating to worker LLMs\n",
      "- Evaluator-Optimizer: One LLM generating responses while another provides evaluation and feedback in a loop\n",
      "\n",
      "4. Implementation:\n",
      "- Can be used with any chat model that supports structured outputs and tool calling\n",
      "- Offers both Graph API and Functional API approaches for building workflows\n",
      "- Provides pre-built methods for common patterns (like create_react_agent)\n",
      "\n",
      "The documentation shows that LangGraph is designed to be a comprehensive framework for building structured LLM applications with emphasis on workflow management, agent creation, and system orchestration.\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Example usage of the LangGraph RAG system.\"\"\"\n",
    "    # Initialize the RAG system\n",
    "    rag_system = LangGraphRAG()\n",
    "    \n",
    "    # Set up the system (loads existing vectorstore or creates new one)\n",
    "    rag_system.setup_rag_system()\n",
    "    \n",
    "    # Example queries\n",
    "    questions = [\n",
    "        \"What is LangGraph?\",\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"LangGraph RAG System - Example Queries\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for question in questions:\n",
    "        print(f\"\\nQ: {question}\")\n",
    "        print(\"-\" * 30)\n",
    "        try:\n",
    "            answer = rag_system.query(question)\n",
    "            print(f\"A: {answer}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
